{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import gc\n",
    "\n",
    "BASE = './'\n",
    "    \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from torchvision import datasets, transforms, models\n",
    "import timm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Data prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((122265, 7), (25080, 3), (25080, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(BASE + 'train.csv')\n",
    "test = pd.read_csv(BASE + 'test.csv')\n",
    "sub = pd.read_csv(BASE + 'sample_submission.csv')\n",
    "\n",
    "train.shape, test.shape, sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"dcount\"] = train.groupby(['cfips'])['row_id'].cumcount()\n",
    "train['county_code'] = train['cfips'].apply(lambda x: str(x)[:-3])\n",
    "train['state_code'] = train['cfips'].apply(lambda x: str(x)[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>dcount</th>\n",
       "      <th>microbusiness_density</th>\n",
       "      <th>cfips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>001</td>\n",
       "      <td>0</td>\n",
       "      <td>3.007682</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>001</td>\n",
       "      <td>1</td>\n",
       "      <td>2.884870</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>001</td>\n",
       "      <td>2</td>\n",
       "      <td>3.055843</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>001</td>\n",
       "      <td>3</td>\n",
       "      <td>2.993233</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>001</td>\n",
       "      <td>4</td>\n",
       "      <td>2.993233</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  county_code state_code  dcount  microbusiness_density  cfips\n",
       "0           1        001       0               3.007682   1001\n",
       "1           1        001       1               2.884870   1001\n",
       "2           1        001       2               3.055843   1001\n",
       "3           1        001       3               2.993233   1001\n",
       "4           1        001       4               2.993233   1001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[['county_code', 'state_code', 'dcount', 'microbusiness_density', 'cfips']]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>county_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>cfips</th>\n",
       "      <th colspan=\"18\" halign=\"left\">microbusiness_density</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dcount</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>001</td>\n",
       "      <td>1001</td>\n",
       "      <td>3.007682</td>\n",
       "      <td>2.884870</td>\n",
       "      <td>3.055843</td>\n",
       "      <td>2.993233</td>\n",
       "      <td>2.993233</td>\n",
       "      <td>2.969090</td>\n",
       "      <td>2.909326</td>\n",
       "      <td>...</td>\n",
       "      <td>3.296781</td>\n",
       "      <td>3.334431</td>\n",
       "      <td>3.336785</td>\n",
       "      <td>3.372082</td>\n",
       "      <td>3.313253</td>\n",
       "      <td>3.346197</td>\n",
       "      <td>3.437971</td>\n",
       "      <td>3.423852</td>\n",
       "      <td>3.442677</td>\n",
       "      <td>3.463856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>003</td>\n",
       "      <td>1003</td>\n",
       "      <td>7.239156</td>\n",
       "      <td>7.290936</td>\n",
       "      <td>7.425439</td>\n",
       "      <td>7.426071</td>\n",
       "      <td>7.470274</td>\n",
       "      <td>7.413655</td>\n",
       "      <td>7.282522</td>\n",
       "      <td>...</td>\n",
       "      <td>7.733397</td>\n",
       "      <td>7.823300</td>\n",
       "      <td>7.945311</td>\n",
       "      <td>7.979170</td>\n",
       "      <td>7.907365</td>\n",
       "      <td>8.507496</td>\n",
       "      <td>8.573463</td>\n",
       "      <td>8.491150</td>\n",
       "      <td>8.341701</td>\n",
       "      <td>8.359798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>005</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.073138</td>\n",
       "      <td>0.995794</td>\n",
       "      <td>1.160149</td>\n",
       "      <td>1.000628</td>\n",
       "      <td>1.000628</td>\n",
       "      <td>1.027229</td>\n",
       "      <td>1.022314</td>\n",
       "      <td>...</td>\n",
       "      <td>1.186629</td>\n",
       "      <td>1.206827</td>\n",
       "      <td>1.196728</td>\n",
       "      <td>1.191678</td>\n",
       "      <td>1.186629</td>\n",
       "      <td>1.191678</td>\n",
       "      <td>1.216926</td>\n",
       "      <td>1.196728</td>\n",
       "      <td>1.206827</td>\n",
       "      <td>1.232074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>007</td>\n",
       "      <td>1007</td>\n",
       "      <td>1.310777</td>\n",
       "      <td>1.305176</td>\n",
       "      <td>1.254761</td>\n",
       "      <td>1.254761</td>\n",
       "      <td>1.265965</td>\n",
       "      <td>1.253638</td>\n",
       "      <td>1.248041</td>\n",
       "      <td>...</td>\n",
       "      <td>1.214165</td>\n",
       "      <td>1.236650</td>\n",
       "      <td>1.264755</td>\n",
       "      <td>1.253513</td>\n",
       "      <td>1.247892</td>\n",
       "      <td>1.275998</td>\n",
       "      <td>1.326588</td>\n",
       "      <td>1.292861</td>\n",
       "      <td>1.315346</td>\n",
       "      <td>1.287240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>009</td>\n",
       "      <td>1009</td>\n",
       "      <td>1.544148</td>\n",
       "      <td>1.575892</td>\n",
       "      <td>1.546415</td>\n",
       "      <td>1.573625</td>\n",
       "      <td>1.555485</td>\n",
       "      <td>1.573217</td>\n",
       "      <td>1.536999</td>\n",
       "      <td>...</td>\n",
       "      <td>1.752923</td>\n",
       "      <td>1.777708</td>\n",
       "      <td>1.797986</td>\n",
       "      <td>1.764189</td>\n",
       "      <td>1.748417</td>\n",
       "      <td>1.773202</td>\n",
       "      <td>1.831783</td>\n",
       "      <td>1.836289</td>\n",
       "      <td>1.852060</td>\n",
       "      <td>1.831783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130</th>\n",
       "      <td>9</td>\n",
       "      <td>007</td>\n",
       "      <td>9007</td>\n",
       "      <td>7.032581</td>\n",
       "      <td>6.923002</td>\n",
       "      <td>6.941015</td>\n",
       "      <td>6.943266</td>\n",
       "      <td>6.918499</td>\n",
       "      <td>6.948894</td>\n",
       "      <td>6.869358</td>\n",
       "      <td>...</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>6.891045</td>\n",
       "      <td>6.926119</td>\n",
       "      <td>6.943284</td>\n",
       "      <td>6.840299</td>\n",
       "      <td>6.857463</td>\n",
       "      <td>6.909701</td>\n",
       "      <td>6.885075</td>\n",
       "      <td>6.889552</td>\n",
       "      <td>6.912686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3131</th>\n",
       "      <td>9</td>\n",
       "      <td>009</td>\n",
       "      <td>9009</td>\n",
       "      <td>6.388350</td>\n",
       "      <td>6.256700</td>\n",
       "      <td>6.211010</td>\n",
       "      <td>6.299460</td>\n",
       "      <td>6.273833</td>\n",
       "      <td>6.277275</td>\n",
       "      <td>6.177921</td>\n",
       "      <td>...</td>\n",
       "      <td>6.753906</td>\n",
       "      <td>6.806637</td>\n",
       "      <td>6.874601</td>\n",
       "      <td>7.074098</td>\n",
       "      <td>7.025469</td>\n",
       "      <td>7.016095</td>\n",
       "      <td>7.112914</td>\n",
       "      <td>7.049198</td>\n",
       "      <td>7.046268</td>\n",
       "      <td>7.033818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3132</th>\n",
       "      <td>9</td>\n",
       "      <td>011</td>\n",
       "      <td>9011</td>\n",
       "      <td>4.404949</td>\n",
       "      <td>4.377251</td>\n",
       "      <td>4.374019</td>\n",
       "      <td>4.392946</td>\n",
       "      <td>4.381405</td>\n",
       "      <td>4.388145</td>\n",
       "      <td>4.304685</td>\n",
       "      <td>...</td>\n",
       "      <td>4.601114</td>\n",
       "      <td>4.615542</td>\n",
       "      <td>4.636951</td>\n",
       "      <td>4.631366</td>\n",
       "      <td>4.581567</td>\n",
       "      <td>4.624385</td>\n",
       "      <td>4.704437</td>\n",
       "      <td>4.685820</td>\n",
       "      <td>4.704902</td>\n",
       "      <td>4.714211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3133</th>\n",
       "      <td>9</td>\n",
       "      <td>013</td>\n",
       "      <td>9013</td>\n",
       "      <td>6.488734</td>\n",
       "      <td>6.367856</td>\n",
       "      <td>6.379944</td>\n",
       "      <td>6.347710</td>\n",
       "      <td>6.404925</td>\n",
       "      <td>6.320415</td>\n",
       "      <td>6.260864</td>\n",
       "      <td>...</td>\n",
       "      <td>4.379989</td>\n",
       "      <td>4.400026</td>\n",
       "      <td>4.424069</td>\n",
       "      <td>4.443304</td>\n",
       "      <td>4.427275</td>\n",
       "      <td>4.444106</td>\n",
       "      <td>4.529862</td>\n",
       "      <td>4.476966</td>\n",
       "      <td>4.500208</td>\n",
       "      <td>4.492995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3134</th>\n",
       "      <td>9</td>\n",
       "      <td>015</td>\n",
       "      <td>9015</td>\n",
       "      <td>3.702548</td>\n",
       "      <td>3.754211</td>\n",
       "      <td>3.748830</td>\n",
       "      <td>3.796188</td>\n",
       "      <td>3.887675</td>\n",
       "      <td>3.932530</td>\n",
       "      <td>3.955106</td>\n",
       "      <td>...</td>\n",
       "      <td>3.524116</td>\n",
       "      <td>3.492040</td>\n",
       "      <td>3.519839</td>\n",
       "      <td>3.538016</td>\n",
       "      <td>3.524116</td>\n",
       "      <td>3.521978</td>\n",
       "      <td>3.574369</td>\n",
       "      <td>3.552985</td>\n",
       "      <td>3.567954</td>\n",
       "      <td>3.566885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3135 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       county_code state_code cfips microbusiness_density                      \\\n",
       "dcount                                                  0         1         2   \n",
       "0                1        001  1001              3.007682  2.884870  3.055843   \n",
       "1                1        003  1003              7.239156  7.290936  7.425439   \n",
       "2                1        005  1005              1.073138  0.995794  1.160149   \n",
       "3                1        007  1007              1.310777  1.305176  1.254761   \n",
       "4                1        009  1009              1.544148  1.575892  1.546415   \n",
       "...            ...        ...   ...                   ...       ...       ...   \n",
       "3130             9        007  9007              7.032581  6.923002  6.941015   \n",
       "3131             9        009  9009              6.388350  6.256700  6.211010   \n",
       "3132             9        011  9011              4.404949  4.377251  4.374019   \n",
       "3133             9        013  9013              6.488734  6.367856  6.379944   \n",
       "3134             9        015  9015              3.702548  3.754211  3.748830   \n",
       "\n",
       "                                                ...                      \\\n",
       "dcount         3         4         5         6  ...        29        30   \n",
       "0       2.993233  2.993233  2.969090  2.909326  ...  3.296781  3.334431   \n",
       "1       7.426071  7.470274  7.413655  7.282522  ...  7.733397  7.823300   \n",
       "2       1.000628  1.000628  1.027229  1.022314  ...  1.186629  1.206827   \n",
       "3       1.254761  1.265965  1.253638  1.248041  ...  1.214165  1.236650   \n",
       "4       1.573625  1.555485  1.573217  1.536999  ...  1.752923  1.777708   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "3130    6.943266  6.918499  6.948894  6.869358  ...  6.900000  6.891045   \n",
       "3131    6.299460  6.273833  6.277275  6.177921  ...  6.753906  6.806637   \n",
       "3132    4.392946  4.381405  4.388145  4.304685  ...  4.601114  4.615542   \n",
       "3133    6.347710  6.404925  6.320415  6.260864  ...  4.379989  4.400026   \n",
       "3134    3.796188  3.887675  3.932530  3.955106  ...  3.524116  3.492040   \n",
       "\n",
       "                                                                              \\\n",
       "dcount        31        32        33        34        35        36        37   \n",
       "0       3.336785  3.372082  3.313253  3.346197  3.437971  3.423852  3.442677   \n",
       "1       7.945311  7.979170  7.907365  8.507496  8.573463  8.491150  8.341701   \n",
       "2       1.196728  1.191678  1.186629  1.191678  1.216926  1.196728  1.206827   \n",
       "3       1.264755  1.253513  1.247892  1.275998  1.326588  1.292861  1.315346   \n",
       "4       1.797986  1.764189  1.748417  1.773202  1.831783  1.836289  1.852060   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "3130    6.926119  6.943284  6.840299  6.857463  6.909701  6.885075  6.889552   \n",
       "3131    6.874601  7.074098  7.025469  7.016095  7.112914  7.049198  7.046268   \n",
       "3132    4.636951  4.631366  4.581567  4.624385  4.704437  4.685820  4.704902   \n",
       "3133    4.424069  4.443304  4.427275  4.444106  4.529862  4.476966  4.500208   \n",
       "3134    3.519839  3.538016  3.524116  3.521978  3.574369  3.552985  3.567954   \n",
       "\n",
       "                  \n",
       "dcount        38  \n",
       "0       3.463856  \n",
       "1       8.359798  \n",
       "2       1.232074  \n",
       "3       1.287240  \n",
       "4       1.831783  \n",
       "...          ...  \n",
       "3130    6.912686  \n",
       "3131    7.033818  \n",
       "3132    4.714211  \n",
       "3133    4.492995  \n",
       "3134    3.566885  \n",
       "\n",
       "[3135 rows x 42 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pt = pd.pivot_table(train, index=['county_code', 'state_code', 'cfips'], columns=['dcount']).reset_index()\n",
    "train_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_122490/1972351883.py:1: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  train_pt = train_pt.sort_values('cfips').drop(['cfips'], axis=1)\n"
     ]
    }
   ],
   "source": [
    "train_pt = train_pt.sort_values('cfips').drop(['cfips'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, predict_borders=(4, 12), min_train_period=12, known_len=39,\n",
    "                 dataset_len=10000, evaluation=False):\n",
    "        self.data = data.copy()\n",
    "        dic_county = {j:i for i, j in enumerate(data['county_code'].unique())}\n",
    "        data['county_code'] = data['county_code'].replace(dic_county)\n",
    "        dic_county = {j:i for i, j in enumerate(data['state_code'].unique())}\n",
    "        data['state_code'] = data['state_code'].replace(dic_county)\n",
    "\n",
    "        self.data_cats = data[['county_code', 'state_code']].values.copy()\n",
    "        self.data_values = data['microbusiness_density'].astype('float32').values.copy()\n",
    "        self.n_places = len(data)\n",
    "        self.predict_borders = predict_borders\n",
    "        self.min_train_period = min_train_period\n",
    "        self.known_len = known_len\n",
    "        self.len = dataset_len\n",
    "        self.eval = evaluation\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        if (self.eval==True):\n",
    "            np.random.seed(index+int(10e6)+42)\n",
    "        else:\n",
    "            np.random.seed(index)\n",
    "            \n",
    "        row = np.random.choice(self.n_places)\n",
    "        predict_len = np.random.randint(self.predict_borders[0], self.predict_borders[1]+1)\n",
    "        \n",
    "        predict_from_max = self.known_len - predict_len # predict from (inclusively)\n",
    "        predict_from_min = self.min_train_period\n",
    "        predict_from = np.random.randint(predict_from_min, predict_from_max+1)\n",
    "        \n",
    "        train_from_max = predict_from - self.min_train_period\n",
    "        train_from_min = 0\n",
    "        train_from = np.random.randint(train_from_min, train_from_max+1)\n",
    "        \n",
    "        return self.data_cats[row, 0],\\\n",
    "               self.data_cats[row, 1],\\\n",
    "               self.data_values[row, train_from:predict_from],\\\n",
    "               self.data_values[row, predict_from:predict_from+predict_len],\\\n",
    "               row\n",
    "    \n",
    "        # return torch.Tensor(self.data_cats[row, 0]),\\\n",
    "        #        torch.Tensor(self.data_cats[row, 1]),\\\n",
    "        #        torch.Tensor(self.data_values[row, train_from:predict_from]),\\\n",
    "        #        torch.Tensor(self.data_values[row, predict_from:predict_from+predict_len]),\\\n",
    "        #        torch.Tensor(row)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    # def __getshape__(self):\n",
    "    #     return (self.__len__(), *self.__getitem__(0)[0].shape)\n",
    "    \n",
    "    def __getsize__(self):\n",
    "        return (self.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset_Test(Dataset):\n",
    "    def __init__(self, data, predict_periods=8, train_periods=12, known_len=39):\n",
    "        self.data = data.copy()\n",
    "        dic_county = {j:i for i, j in enumerate(data['county_code'].unique())}\n",
    "        data['county_code'] = data['county_code'].replace(dic_county)\n",
    "        dic_county = {j:i for i, j in enumerate(data['state_code'].unique())}\n",
    "        data['state_code'] = data['state_code'].replace(dic_county)\n",
    "\n",
    "        self.data_cats = data[['county_code', 'state_code']].values.copy()\n",
    "        self.data_values = data['microbusiness_density'].astype('float32').values.copy()\n",
    "        self.predict_periods = predict_periods\n",
    "        self.train_periods = train_periods\n",
    "        self.known_len = known_len\n",
    "        self.len = len(data)\n",
    "        # np.random.seed(random_state)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        row = index\n",
    "        train_from = self.known_len - self.train_periods\n",
    "        \n",
    "        return self.data_cats[row, 0],\\\n",
    "               self.data_cats[row, 1],\\\n",
    "               self.data_values[row, train_from:],\\\n",
    "               -1,\\\n",
    "               row\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    # def __getshape__(self):\n",
    "    #     return (self.__len__(), *self.__getitem__(0)[0].shape)\n",
    "    \n",
    "    def __getsize__(self):\n",
    "        return (self.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "# EOS_token = 1 # no need for EOS token - we always knew the length of target\n",
    "\n",
    "MIN_PRED_LENGTH = 2\n",
    "MAX_PRED_LENGTH = 4\n",
    "MIN_TRAIN_PERIOD = 12\n",
    "DATASET_TRAIN_LEN = 100000\n",
    "DATASET_EVAL_LEN = 10000\n",
    "\n",
    "MAX_LENGTH = 39\n",
    "# -4+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = CustomDataset(train_pt,\n",
    "                              predict_borders = (MIN_PRED_LENGTH, MAX_PRED_LENGTH),\n",
    "                              min_train_period = MIN_TRAIN_PERIOD,\n",
    "                              dataset_len = DATASET_TRAIN_LEN)\n",
    "\n",
    "dataset_eval = CustomDataset(train_pt,\n",
    "                             predict_borders = (2, 4),\n",
    "                             min_train_period = MIN_TRAIN_PERIOD,\n",
    "                             dataset_len = DATASET_EVAL_LEN,\n",
    "                             evaluation = True)\n",
    "\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=1, num_workers=8)\n",
    "dataloader_eval = torch.utils.data.DataLoader(dataset_eval, batch_size=1, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_test = CustomDataset_Test(train_pt, train_periods=MIN_TRAIN_PERIOD)\n",
    "# dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Classes & Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Simplified seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size_county, input_size_state, hidden_size, use_cats=False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_cats = use_cats\n",
    "\n",
    "        self.embedding_county = nn.Embedding(input_size_county, 8)\n",
    "        self.embedding_state = nn.Embedding(input_size_state, 16)\n",
    "        \n",
    "        if (self.use_cats==True):\n",
    "            self.rnn = nn.RNN(1+8+16, hidden_size)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(1, hidden_size)\n",
    "\n",
    "    def forward(self, county, state, input, hidden):\n",
    "        output = input.view(1, 1, -1)\n",
    "        if (self.use_cats==True):\n",
    "            embedded_county = self.embedding_county(county).view(1, 1, -1)\n",
    "            embedded_state = self.embedding_state(state).view(1, 1, -1)\n",
    "            output = torch.cat([embedded_county, embedded_state, output], dim=-1)\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size_county, input_size_state, hidden_size, output_size=1, use_cats=False):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_cats = use_cats\n",
    "\n",
    "        self.embedding_county = nn.Embedding(input_size_county, 8)\n",
    "        self.embedding_state = nn.Embedding(input_size_state, 16)\n",
    "        \n",
    "        if (self.use_cats==True):\n",
    "            self.rnn = nn.RNN(1+8+16, hidden_size)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(1, hidden_size)\n",
    "            \n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, county, state, input, hidden):\n",
    "\n",
    "        output = input.view(1, 1, -1)\n",
    "        if (self.use_cats==True):\n",
    "            embedded_county = self.embedding_county(county).view(1, 1, -1)\n",
    "            embedded_state = self.embedding_state(state).view(1, 1, -1)\n",
    "            output = torch.cat([embedded_county, embedded_state, output], dim=-1)\n",
    "            output = F.relu(output)\n",
    "        \n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        output = self.out(output[0]) #!!!\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(county_tensor, state_tensor, input_tensor, target_tensor,\n",
    "          encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            county_tensor, state_tensor, input_tensor[ei], encoder_hidden)\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device, dtype=torch.float32)\n",
    "    decoder_hidden = encoder_hidden # simple seq2seq model with only contex vector transmitting\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                county_tensor, state_tensor, decoder_input, decoder_hidden)\n",
    "            \n",
    "            loss += criterion(decoder_output.squeeze_(), target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                county_tensor, state_tensor, decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output.squeeze_(), target_tensor[di])\n",
    "            decoder_input = decoder_output\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(county_tensor, state_tensor, input_tensor, target_tensor,\n",
    "             encoder, decoder, criterion, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "        input_length = input_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "        # input_length = len(input_tensor)\n",
    "        # target_length = len(target_tensor)\n",
    "        \n",
    "        loss = 0\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(\n",
    "                county_tensor, state_tensor, input_tensor[ei], encoder_hidden)\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device, dtype=torch.float32)  # SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                county_tensor, state_tensor, decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output.squeeze_(), target_tensor[di])\n",
    "            decoder_input = decoder_output\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder, decoder, loader_test, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        preds = torch.zeros(len(loader_test), max_length)\n",
    "        \n",
    "        row = 0\n",
    "        for iter in tqdm(loader_test):\n",
    "            county_tensor, state_tensor, input_tensor, target_tensor = \\\n",
    "                iter[0].squeeze_().to(device), iter[1].squeeze_().to(device), iter[2].squeeze_().to(device), iter[3].squeeze_().to(device)\n",
    "        \n",
    "            encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "            input_length = input_tensor.size(0)\n",
    "            target_length = max_length\n",
    "\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(\n",
    "                    county_tensor, state_tensor, input_tensor[ei], encoder_hidden)\n",
    "\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device, dtype=torch.float32)  # SOS\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    county_tensor, state_tensor, decoder_input, decoder_hidden)\n",
    "                preds[row, di] = decoder_output\n",
    "                decoder_input = decoder_output\n",
    "            row += 1\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMAPE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, y_pred: torch.Tensor, y: torch.Tensor) -> float:\n",
    "        return 2*(y - y_pred).abs() / (y.abs() + y_pred.abs())\n",
    "        \n",
    "#         e = torch.abs(y.view_as(y_pred) - y_pred) / torch.abs(y.view_as(y_pred))\n",
    "#         return 100.0 * torch.median(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, loader_train, loader_eval, epochs=1,\n",
    "               print_every=10000, plot_every=10000, learning_rate=0.001):\n",
    "    \n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # criterion = nn.L1Loss()\n",
    "    criterion = SMAPE()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        counter = 0\n",
    "    \n",
    "        for iter in tqdm(loader_train):\n",
    "            county_tensor, state_tensor, input_tensor, target_tensor = \\\n",
    "            iter[0].squeeze_().to(device), iter[1].squeeze_().to(device), iter[2].squeeze_().to(device), iter[3].squeeze_().to(device)\n",
    "\n",
    "            loss = train(county_tensor, state_tensor, input_tensor, target_tensor,\n",
    "                         encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "            counter +=1\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                \n",
    "                for iter_eval in loader_eval:\n",
    "                    county_tensor, state_tensor, input_tensor, target_tensor = \\\n",
    "                    iter_eval[0].squeeze_().to(device), iter_eval[1].squeeze_().to(device), iter_eval[2].squeeze_().to(device), iter_eval[3].squeeze_().to(device)\n",
    "                \n",
    "                    loss_eval = evaluate(county_tensor, state_tensor, input_tensor, target_tensor,\n",
    "                                         encoder, decoder, criterion)\n",
    "\n",
    "                print('train loss average..', print_loss_avg, '\\n',\n",
    "                      'eval loss average..', loss_eval, '\\n',\n",
    "                      'epoch..', epoch, 'progress..', counter/len(loader_train)*100)\n",
    "                \n",
    "                \n",
    "                \n",
    "#             if counter % plot_every == 0:\n",
    "#                 plot_loss_avg = plot_loss_total / plot_every\n",
    "#                 plot_losses.append(plot_loss_avg)\n",
    "#                 plot_loss_total = 0\n",
    "\n",
    "#         showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Attention seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size_county, input_size_state, hidden_size, use_cats=False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_cats = use_cats\n",
    "\n",
    "        self.embedding_county = nn.Embedding(input_size_county, 8)\n",
    "        self.embedding_state = nn.Embedding(input_size_state, 16)\n",
    "        \n",
    "        if (self.use_cats==True):\n",
    "            self.rnn = nn.RNN(1+8+16, hidden_size)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(1, hidden_size)\n",
    "\n",
    "    def forward(self, county, state, input, hidden):\n",
    "        output = input.view(1, 1, -1)\n",
    "        if (self.use_cats==True):\n",
    "            embedded_county = self.embedding_county(county).view(1, 1, -1)\n",
    "            embedded_state = self.embedding_state(state).view(1, 1, -1)\n",
    "            output = torch.cat([embedded_county, embedded_state, output], dim=-1)\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size_county, input_size_state, hidden_size,\n",
    "                 output_size=1, use_cats=False, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_cats = use_cats\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding_county = nn.Embedding(input_size_county, 8)\n",
    "        self.embedding_state = nn.Embedding(input_size_state, 16)\n",
    "        \n",
    "        if (self.use_cats==True):\n",
    "            rnn_input = 1+8+16\n",
    "        else:\n",
    "            rnn_input = 1\n",
    "        \n",
    "        self.attn = nn.Linear(self.hidden_size + rnn_input, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size + rnn_input, self.hidden_size) #!!!\n",
    "        \n",
    "        self.rnn = nn.RNN(self.hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, county, state, input, hidden, encoder_outputs):\n",
    "        # output = F.relu(output)\n",
    "        output = input.view(1, 1, -1)\n",
    "        if (self.use_cats==True):\n",
    "            embedded_county = self.embedding_county(county).view(1, 1, -1)\n",
    "            embedded_state = self.embedding_state(state).view(1, 1, -1)\n",
    "            output = torch.cat([embedded_county, embedded_state, output], dim=-1)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((output[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = torch.cat((output[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        \n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        output = self.out(output[0]) #!!!\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(county_tensor, state_tensor, input_tensor, target_tensor,\n",
    "          encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            county_tensor, state_tensor, input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0] # stow hiddens in rows\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device, dtype=torch.float32)\n",
    "    decoder_hidden = encoder_hidden # last hidden from encoder becomes first for decoder\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                county_tensor, state_tensor, decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            loss += criterion(decoder_output.squeeze_(), target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                county_tensor, state_tensor, decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output.squeeze_(), target_tensor[di])\n",
    "            decoder_input = decoder_output\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(county_tensor, state_tensor, input_tensor, target_tensor,\n",
    "             encoder, decoder, criterion, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "        input_length = input_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "        \n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        \n",
    "        loss = 0\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(\n",
    "                county_tensor, state_tensor, input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0] # stow hiddens in rows\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device, dtype=torch.float32)  # SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                county_tensor, state_tensor, decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output.squeeze_(), target_tensor[di])\n",
    "            decoder_input = decoder_output\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder, decoder, loader_test, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        preds = torch.zeros(len(loader_test), max_length)\n",
    "        \n",
    "        row = 0\n",
    "        for iter in tqdm(loader_test):\n",
    "            county_tensor, state_tensor, input_tensor, target_tensor = \\\n",
    "                iter[0].squeeze_().to(device), iter[1].squeeze_().to(device), iter[2].squeeze_().to(device), iter[3].squeeze_().to(device)\n",
    "        \n",
    "            encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "            input_length = input_tensor.size(0)\n",
    "            target_length = max_length\n",
    "            \n",
    "            encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(\n",
    "                    county_tensor, state_tensor, input_tensor[ei], encoder_hidden)\n",
    "                encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device, dtype=torch.float32)  # SOS\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    county_tensor, state_tensor, decoder_input, decoder_hidden,  encoder_outputs)\n",
    "                preds[row, di] = decoder_output\n",
    "                decoder_input = decoder_output\n",
    "            row += 1\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, loader_train, loader_eval, epochs=1,\n",
    "               print_every=10000, plot_every=10000, learning_rate=0.001):\n",
    "    \n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # criterion = nn.L1Loss()\n",
    "    criterion = SMAPE()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        counter = 0\n",
    "    \n",
    "        for iter in tqdm(loader_train):\n",
    "            county_tensor, state_tensor, input_tensor, target_tensor = \\\n",
    "            iter[0].squeeze_().to(device), iter[1].squeeze_().to(device), iter[2].squeeze_().to(device), iter[3].squeeze_().to(device)\n",
    "\n",
    "            loss = train(county_tensor, state_tensor, input_tensor, target_tensor,\n",
    "                         encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "            counter +=1\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                \n",
    "                for iter_eval in loader_eval:\n",
    "                    county_tensor, state_tensor, input_tensor, target_tensor = \\\n",
    "                    iter_eval[0].squeeze_().to(device), iter_eval[1].squeeze_().to(device), iter_eval[2].squeeze_().to(device), iter_eval[3].squeeze_().to(device)\n",
    "                \n",
    "                    loss_eval = evaluate(county_tensor, state_tensor, input_tensor, target_tensor,\n",
    "                                         encoder, decoder, criterion)\n",
    "\n",
    "                print('train loss average..', print_loss_avg, '\\n',\n",
    "                      'eval loss average..', loss_eval, '\\n',\n",
    "                      'epoch..', epoch, 'progress..', counter/len(loader_train)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9007f43945164b7f89dbb7a7bb2a4f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "county_n = 51\n",
    "state_n = 391\n",
    "\n",
    "encoder = EncoderRNN(county_n, state_n, hidden_size, use_cats=False).to(device)\n",
    "decoder = AttnDecoderRNN(county_n, state_n, hidden_size, use_cats=False).to(device)\n",
    "\n",
    "trainIters(encoder, decoder, dataloader_train, dataloader_eval,\n",
    "           print_every=10000, learning_rate=5e-5) # 1.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e14382af43c4646bb0d1e7353188063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss average.. 0.06268013452637375 \n",
      " eval loss average.. 0.12634003162384033 \n",
      " epoch.. 0 progress.. 10.0\n",
      "train loss average.. 0.04266209104622722 \n",
      " eval loss average.. 0.10197688639163971 \n",
      " epoch.. 0 progress.. 20.0\n",
      "train loss average.. 0.03807817681238331 \n",
      " eval loss average.. 0.09003733098506927 \n",
      " epoch.. 0 progress.. 30.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m encoder \u001b[38;5;241m=\u001b[39m EncoderRNN(county_n, state_n, hidden_size, use_cats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m decoder \u001b[38;5;241m=\u001b[39m AttnDecoderRNN(county_n, state_n, hidden_size, use_cats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrainIters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m           \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, loader_train, loader_eval, epochs, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tqdm(loader_train):\n\u001b[1;32m     18\u001b[0m     county_tensor, state_tensor, input_tensor, target_tensor \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28miter\u001b[39m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze_()\u001b[38;5;241m.\u001b[39mto(device), \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28miter\u001b[39m[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze_()\u001b[38;5;241m.\u001b[39mto(device), \u001b[38;5;28miter\u001b[39m[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze_()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train(county_tensor, state_tensor, input_tensor, target_tensor,\n\u001b[1;32m     22\u001b[0m                  encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[1;32m     23\u001b[0m     print_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 512\n",
    "county_n = 51\n",
    "state_n = 391\n",
    "\n",
    "encoder = EncoderRNN(county_n, state_n, hidden_size, use_cats=True).to(device)\n",
    "decoder = AttnDecoderRNN(county_n, state_n, hidden_size, use_cats=True).to(device)\n",
    "\n",
    "trainIters(encoder, decoder, dataloader_train, dataloader_eval,\n",
    "           print_every=100000, learning_rate=5e-5) # 1.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73f8b1108d744c5a51180dba791ae04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss average.. 0.23135891461985697 \n",
      " eval loss average.. 0.03474698215723038 \n",
      " epoch.. 0 progress.. 10.0\n",
      "train loss average.. 0.07545649327924463 \n",
      " eval loss average.. 0.04133499041199684 \n",
      " epoch.. 0 progress.. 20.0\n",
      "train loss average.. 0.06560148647213339 \n",
      " eval loss average.. 0.01970205083489418 \n",
      " epoch.. 0 progress.. 30.0\n",
      "train loss average.. 0.06198505383321821 \n",
      " eval loss average.. 0.05153324082493782 \n",
      " epoch.. 0 progress.. 40.0\n",
      "train loss average.. 0.06111650049594636 \n",
      " eval loss average.. 0.0364997535943985 \n",
      " epoch.. 0 progress.. 50.0\n",
      "train loss average.. 0.05767028022838144 \n",
      " eval loss average.. 0.03931524604558945 \n",
      " epoch.. 0 progress.. 60.0\n",
      "train loss average.. 0.05574559723702919 \n",
      " eval loss average.. 0.011198178865015507 \n",
      " epoch.. 0 progress.. 70.0\n",
      "train loss average.. 0.0561930436728772 \n",
      " eval loss average.. 0.016934365034103394 \n",
      " epoch.. 0 progress.. 80.0\n",
      "train loss average.. 0.05390153816410975 \n",
      " eval loss average.. 0.019464142620563507 \n",
      " epoch.. 0 progress.. 90.0\n",
      "train loss average.. 0.05405296184435166 \n",
      " eval loss average.. 0.009580781683325768 \n",
      " epoch.. 0 progress.. 100.0\n"
     ]
    }
   ],
   "source": [
    "# hidden_size = 256\n",
    "# county_n = 51\n",
    "# state_n = 391\n",
    "# # MAX_LENGTH = 15\n",
    "# encoder = EncoderRNN(county_n, state_n, hidden_size, use_cats=True).to(device)\n",
    "# decoder = DecoderRNN(county_n, state_n, hidden_size, use_cats=False).to(device)\n",
    "\n",
    "# trainIters(encoder, decoder, dataloader_train, dataloader_eval,\n",
    "#            print_every=10000, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = CustomDataset_Test(train_pt, train_periods=12)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f91bc44025d4c47a9a4d7d03a792a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = test(encoder, decoder, loader_test=dataloader_test, max_length=39)\n",
    "for i in range(8):\n",
    "    sub.iloc[0+i*3135:3135+i*3135, 1] = preds[:, i].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.4891, 3.4919, 3.5194,  ..., 3.5835, 3.6028, 3.6217],\n",
       "        [8.3997, 8.3836, 8.4299,  ..., 8.5319, 8.5629, 8.5939],\n",
       "        [1.2431, 1.2529, 1.2662,  ..., 1.2997, 1.3101, 1.3203],\n",
       "        ...,\n",
       "        [4.0356, 4.0348, 4.0658,  ..., 4.1359, 4.1568, 4.1772],\n",
       "        [3.1532, 3.1561, 3.1804,  ..., 3.2409, 3.2595, 3.2778],\n",
       "        [1.7996, 1.8060, 1.8202,  ..., 1.8540, 1.8641, 1.8739]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:, :8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
